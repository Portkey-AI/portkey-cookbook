{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saif-Shines/pk-cookbook/blob/autogen-portkey-klyst-170/product/observability_and_resilience_essentials_for_autogen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observability and Resilience Essentials for Autogen\n",
        "\n",
        "Autogen lets you bring collaborative conversations among LLMs, Humans, and Tools to reason together and take action. All of those Agents can be remarkably composed together to be the powerful components of your LLM apps.\n",
        "\n",
        "On the other hand, Portkey integrates production-grade monitoring and resiliency capabilities into the Autogen workflows so you have all the insights you need to put your LLM based apps in produciton.\n",
        "\n",
        "Some include:\n",
        "\n",
        "1. Calling 100+ LLMs (open & closed)\n",
        "2. Logging & analyzing LLM usage\n",
        "3. Caching responses\n",
        "4. Automating fallbacks, retries, timeouts, and load balancing\n",
        "\n",
        "Let's see how you can leverage Autogen and Portkey.\n"
      ],
      "metadata": {
        "id": "C80RNiMSKlPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install the client sdk for Portkey and Autogen using the following command"
      ],
      "metadata": {
        "id": "JH0c06dPKt4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LsgbYK9EWnT"
      },
      "outputs": [],
      "source": [
        "!pip install pyautogen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlMKcR89UOTH"
      },
      "outputs": [],
      "source": [
        "!pip install portkey-ai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the SDKs are installed, you need authentication keys to instantiate them.\n",
        "\n",
        "1. Login to the [Portkey app](https://www.portkey.ai/).\n",
        "2. To integrate OpenAI with Portkey, add your OpenAI API key as a [Virtual Key](https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/virtual-keys).\n",
        "3. This will give you a disposable key that you can use and rotate instead of directly using the OpenAI API key."
      ],
      "metadata": {
        "id": "GGppmA82K5Wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrate Autogen with Portkey\n",
        "\n",
        "Autogen lets you define a <code>[config_list](https://microsoft.github.io/autogen/docs/topics/llm_configuration#introduction-to-config_list)</code> in which Portkey can be listed seamlessly.\n"
      ],
      "metadata": {
        "id": "9699vsDUK9fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJh4ZyO5R0oB"
      },
      "outputs": [],
      "source": [
        "import autogen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx6xoAaiU-Ny"
      },
      "outputs": [],
      "source": [
        "from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tI5FJ2SRaUT"
      },
      "outputs": [],
      "source": [
        "config_list = [\n",
        "    {\n",
        "        \"api_key\": '<ignored>',\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"max_tokens\": 512,\n",
        "        \"base_url\": PORTKEY_GATEWAY_URL,\n",
        "        \"api_type\": \"openai\",\n",
        "        \"default_headers\": createHeaders(\n",
        "            api_key = \"PORTKEY_API_KEY\",\n",
        "            virtual_key=\"OPENAI_VIRTUAL_KEY\",\n",
        "            trace_id=\"tracing_autogen\",\n",
        "            metadata={\"_user\": \"USER_ID\"}\n",
        "        )\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top-level `api_key` parameter should be the API key of the LLM provider, which in this case is OpenAI. However, since we changed the `base_url` to be Portkey’s AI gateway, the `virtual_key` takes precedence, and the top-level `api_key` is not respected.\n",
        "\n",
        "The parameters under `default_headers` are sent to Portkey. Hence, `api_key` should be Portkey’s API key, and `trace_id` can be any identifier that helps you filter and view the request logs in the Portkey app.\n"
      ],
      "metadata": {
        "id": "UTfhSOw8LFdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Write Python code and store it in a file\n",
        "\n",
        "Let’s create two Agents using Autogen — LLM Agent (Assistant Agent) and Human Agent (User Proxy Agent)."
      ],
      "metadata": {
        "id": "g_MmtEBELGnw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZPhDTZ2Rqq4"
      },
      "outputs": [],
      "source": [
        "llm_config={\n",
        "    \"config_list\": config_list,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vZ61Ql06Rtzi"
      },
      "outputs": [],
      "source": [
        "assistant = autogen.AssistantAgent(\n",
        "    name=\"Staff Engineer\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"You are very much experienced Python Programmer\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvGuUKMARxTU"
      },
      "outputs": [],
      "source": [
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    code_execution_config={\"work_dir\": \"web\"},\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
        "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We defined the role of an `assistant` as an experienced staff engineer. Meanwhile, `user_proxy` participates in the conversation on your behalf. It replies less than 10 times without needing your input to do the task. To learn more about parameters in detail, visit [Autogen documentation](https://microsoft.github.io/autogen/docs/Getting-Started/).\n",
        "\n",
        "Initiate the conversation to execute the task (s):"
      ],
      "metadata": {
        "id": "ezykdtPMLRwi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDrg5wBbTFb7"
      },
      "outputs": [],
      "source": [
        "task = \"\"\"\n",
        "Write python code to output numbers 1 to 10, and then store the code in a file\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRaST2dETH6Q"
      },
      "outputs": [],
      "source": [
        "user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=task\n",
        ")\n",
        "\n",
        "# See the root directory of this file to find the python file generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KaDK7IqZVcQ"
      },
      "outputs": [],
      "source": [
        "task2 = \"\"\"\n",
        "Change the code in the file you just created to instead output numbers 1 to 200\n",
        "\"\"\"\n",
        "\n",
        "user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=task2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the task — The Python file will be created and updated programmatically in your current working directory.\n"
      ],
      "metadata": {
        "id": "UfAI3zZ7NS7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Switch from OpenAI to Anyscale\n",
        "\n",
        "Porkey’s AI gateway (also [open source](https://github.com/portkey-ai/gateway)) can route your requests to 100+ LLMs using the OpenAI signature you already know. It can let you switch from GPT4 to Mistral by updating just two lines of code.\n",
        "\n",
        "\n",
        "```js\n",
        "config_list = [\n",
        "    {\n",
        "        \"api_key\": '<ignored>',\n",
        "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        \"base_url\": PORTKEY_GATEWAY_URL,\n",
        "        \"api_type\": \"openai\",\n",
        "        \"default_headers\": createHeaders(\n",
        "            api_key = \"xxxxx\",\n",
        "            virtual_key=\"anyscxxxxx65\",\n",
        "            trace_id=\"tracing_autogen\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "The same will apply to all the other providers like Azure, Mistral, Anyscale, Together, and more. See the [integration guides](https://portkey.ai/docs/welcome/integration-guides)!\n",
        "\n",
        "Note that not all the models have capabilities (for example,  function calling) that enable Autogen to be drop-in compatible to accomplish the tasks.\n",
        "\n",
        "\n",
        "## Trace and View Autogen Conversations\n",
        "\n",
        "Portkey lets you inspect every interaction with an LLM on a single control panel in its app. You can surf through the logs, analytics, traces, and much more to get complete insights about your LLM app.\n",
        "\n",
        "Autogen initiates the conversation and gets the task done. With Portkey, you can see tokens used, costs, and inspect all the requests and responses to better optimize your apps.\n",
        "\n",
        "![](https://raw.githubusercontent.com/Portkey-AI/portkey-cookbook/a4ebb6c7cb75f23be01d398cbbbe8fbe669a053a/integrations/images/observability-and-resilience-essentials-for-autogen/1-observability-and-resilience-essentials-for-autogen.gif)\n",
        "\n",
        "## Segment the requests using metadata\n",
        "\n",
        "You can also pass any `{\"key\":\"value\"}` pairs to segment the requests based on the metadata and get granular insights.\n",
        "\n",
        "\n",
        "```js\n",
        "config_list = [\n",
        "    {\n",
        "        \"api_key\": '<ignored>',\n",
        "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        \"max_tokens\": 512,\n",
        "        \"base_url\": PORTKEY_GATEWAY_URL,\n",
        "        \"api_type\": \"openai\",\n",
        "        \"default_headers\": createHeaders(\n",
        "            api_key = \"xxxxx\",\n",
        "            virtual_key=\"anyscale-xxxxx\",\n",
        "            trace_id=\"tracing_autogen\",\n",
        "            metadata={\"_user\": \"USER_ID\"}\n",
        "        )\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "Learn more about [tracing](https://portkey.ai/docs/product/observability-modern-monitoring-for-llms/traces) and [feedback](https://portkey.ai/docs/product/observability-modern-monitoring-for-llms/feedback).\n",
        "\n",
        "\n",
        "## Automatically Handle LLM Failures\n",
        "\n",
        "Portkey can handle temporary LLM downtimes and rate limit errors by applying fallbacks and loadbalancing the requests. To enable this, login to the Portkey app and **Configs** and click **Create**.\n",
        "\n",
        "\n",
        "### 4XX and 5XX errors\n",
        "\n",
        "For example, for setting up a fallback from OpenAI to Anthropic, the Gateway Config would be:\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"strategy\": { \"mode\": \"fallback\" },\n",
        "    \"targets\": [\n",
        "        { \"virtual_key\": \"openai-virtual-key\" },\n",
        "        { \"virtual_key\": \"anthropic-virtual-key\" }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "You can save this Config and get an associated Config ID to pass it as part of `default_headers`\n",
        "\n",
        "```js\n",
        "config_list = [\n",
        "    {\n",
        "        \"api_key\": '<ignored>',\n",
        "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        \"base_url\": PORTKEY_GATEWAY_URL,\n",
        "        \"api_type\": \"openai\",\n",
        "        \"default_headers\": createHeaders(\n",
        "            api_key = \"xxxxx\",\n",
        "            virtual_key=\"annyscale-xxxxx\",\n",
        "            trace_id=\"tracing_autogen\",\n",
        "            config=\"CONFIG_ID\"\n",
        "          )\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "### Handle Ratelimits\n",
        "\n",
        "Similarly, you can loadbalance the requests against multiple LLMs or accounts to prevent any one account from hitting rate limit thresholds.\n",
        "\n",
        "For example, to route your requests between 1 OpenAI and 2 Azure OpenAI accounts:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"strategy\": { \"mode\": \"loadbalance\" },\n",
        "    \"targets\": [\n",
        "        { \"virtual_key\": \"openai-virtual-key\", \"weight\":1 },\n",
        "        { \"virtual_key\": \"azure-virtual-key-1\", \"weight\":1 },\n",
        "        { \"virtual_key\": \"azure-virtual-key-2\", \"weight\":1 }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Learn more such features as semantic caching, automatic retries and request timeout on the [documentation](https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations).\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Autogen combined with Portkey, will give your apps the collaborative power of LLMs and production-grade resiliency. If you have any questions or issues, reach out to us on [Discord](https://portkey.ai/community). On Discord, you will also meet many other practitioners who are putting their LLM apps into production.\n",
        "\n"
      ],
      "metadata": {
        "id": "3IGAnDuQNbwV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmWxsRdIZavIDhWjzeNxfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}