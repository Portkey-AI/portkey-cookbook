{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwQbYhPmmAP0p+yXi3Xsl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saif-Shines/pk-cookbook/blob/portkey-on-google-cookbook-repo-klyst-171/integrations/Resiliency_and_Observability_Essentials_for_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resiliency and Observability Essentials for Gemini\n",
        "\n",
        "Gemini is a series of multimodal generative AI models developed by Google. Gemini models can accept text and images in prompts, depending on what model variation you choose and output text responses. Keeping track of token costs, monitoring, and improving your app can be challenging. With Portkey, you get features that will help you solve these production challenges in minutes.\n",
        "\n",
        "Dive into this guide for a comprehensive view of everything you could do with Portkey to make your Gemini-based app production ready!\n",
        "\n",
        "## Portkey\n",
        "\n",
        "Portkey provides an Observability suite and AI gateway capabilities for your gen-AI apps in production. It allows you to analyze all the logs and get power analytics on top of them. Not only can all your requests be served from the cache, but traffic can also be load-balanced, and fallbacks can be applied to improve reliability and save costs. These are just a few of the many features that Portkey has to offer.\n",
        "\n",
        "\n",
        "## Integration with Gemini\n",
        "\n",
        "The easiest way to get Portkey working for you is through its client SDK.\n"
      ],
      "metadata": {
        "id": "nLxui2kgZyu1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne_BbNF7o6p7"
      },
      "outputs": [],
      "source": [
        "!pip install portkey_ai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the [Portkey API Key](https://portkey.ai/docs/api-reference/authentication#obtaining-your-api-key) and instantiate it to start using it to make chat completion calls.\n"
      ],
      "metadata": {
        "id": "PFmhuYi_aUhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from portkey_ai import Portkey\n",
        "from google.colab import userdata # To work with environment varialbes in collab\n",
        "\n",
        "PORTKEY_API_KEY=userdata.get('PORTKEY_API_KEY')\n",
        "GOOGLE_VIRTUAL_KEY=userdata.get('GOOGLE_VIRTUAL_KEY')\n",
        "\n",
        "portkey = Portkey(\n",
        "    api_key=PORTKEY_API_KEY,\n",
        "    virtual_key=GOOGLE_VIRTUAL_KEY\n",
        ")\n",
        "\n",
        "response = portkey.chat.completions.create(\n",
        "    model=\"gemini-1.0-pro-001\",\n",
        "    messages = [{ \"role\": \"user\", \"content\": \"c'est la vie\" }]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKN3XJubpJLy",
        "outputId": "b473bd0d-656c-419a-e1d7-c67d9b6174fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C'est la vie is a French phrase that means \"such is life.\" It is often used to express resignation or acceptance of a difficult situation. The phrase suggests that life is full of challenges and that it is important to learn to accept the things that you cannot change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Virtual Keys\n",
        "\n",
        "With Portkey Vault, your Google Gemini API Key is stored safely, and a Virtual Key is generated. The Virtual Key has many advantages, such as the ability to rotate keys easily, multiple virtual keys for a single API key, and the option to impose restrictions based on cost, request volume, and user volume.\n",
        "\n",
        "Learn [how to create Virtual Keys](https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/virtual-keys#using-virtual-keys) and use them for your requests.\n",
        "\n",
        "## Observability\n",
        "\n",
        "Portkey can help you keep better track of your information and ensure that you're not missing out on any valuable insights.\n",
        "\n",
        "1. Understand the number of tokens used and the nature of the token-costs attached.\n",
        "2. Know when and what has caused an issue in production and troubleshoot it.\n",
        "3. Analyze the status of each request and replay them as necessary.\n",
        "\n",
        "There’s more to what you can do.\n",
        "\n",
        "### Logging\n",
        "\n",
        "Every request through Portkey to Anyscale will appear as a Log in the **Logs** page. Each log gives insights about the request and response body, timestamps, request timings, tokens, costs, and many more details.\n"
      ],
      "metadata": {
        "id": "Sd_BKmJlafw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = portkey.chat.completions.create(\n",
        "    model=\"gemini-1.0-pro-001\",\n",
        "    messages = [{ \"role\": \"user\", \"content\": \"c'est la vie\" }]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P9yuGSwsgOG",
        "outputId": "db6d8166-c1ca-4a2e-bcde-3b0ada0fd563"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C'est la vie (French pronunciation: ​[sɛ la vi]) is a French phrase that means \"such is life\" or \"that's life\". It is often used to express resignation or acceptance in the face of adversity. The phrase can also be used in a more positive sense, to express the idea that life is full of both good and bad experiences, and that it is important to accept both.\n",
            "\n",
            "The phrase is thought to have originated in the 16th century, and it has been used by many famous people over the years, including Marie Antoinette, Napoleon Bonaparte, and Albert Einstein. Today, the phrase is still commonly used in both French and English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a screenshot of it:\n",
        "![](https://github.com/Saif-Shines/pk-cookbook/blob/portkey-on-google-cookbook-repo-klyst-171/integrations/images/resiliency-and-observability-essentials-for-gemini/1-resiliency-and-observability-essentials-for-gemini.png?raw=true)"
      ],
      "metadata": {
        "id": "yVsZXo-CbRBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Tracing\n",
        "\n",
        "Using Tracing abilities, you can pinpoint and analyze requests throughout their life-cycle and quickly filter through the heap of logs.\n",
        "\n"
      ],
      "metadata": {
        "id": "FaAn6Zy9dSzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = portkey.with_options(\n",
        "  trace_id=\"gemini_and_portkey\" # any trace identifier\n",
        ").chat.completions.create(\n",
        "    model=\"gemini-1.0-pro-001\",\n",
        "    messages = [{ \"role\": \"user\", \"content\": \"Greet me in Japanese!\" }]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQJuKYidtgPw",
        "outputId": "8b2d305c-6902-4725-e892-70d18d91a835"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "こんにちは (Konnichiwa)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a screenshot\n",
        "\n",
        "![](https://github.com/Saif-Shines/pk-cookbook/blob/portkey-on-google-cookbook-repo-klyst-171/integrations/images/resiliency-and-observability-essentials-for-gemini/2-resiliency-and-observability-essentials-for-gemini.png?raw=true)"
      ],
      "metadata": {
        "id": "5EdAwUNgdYVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metadata\n",
        "\n",
        "Segment your requests and analyze them by attaching Metadata. They can describe anything that might be of valuable insight to you. For example, using metadata is the best answer if you want to segment the conversations related to a specific user.\n"
      ],
      "metadata": {
        "id": "QhmiK-gwdsgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = portkey.with_options(\n",
        "  trace_id=\"gemini_and_portkey\",\n",
        "  metadata={\n",
        "    \"_user\": \"3511522\",\n",
        "    \"environment\":\"dev\"\n",
        "  }\n",
        ").chat.completions.create(\n",
        "    model=\"gemini-1.0-pro-001\",\n",
        "    messages = [{ \"role\": \"user\", \"content\": \"Greet me in Japanese!\" }]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIwzXdNCNJCX",
        "outputId": "d3fb2a20-02cf-411f-b19e-bc77605442ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "こんにちは! (Konnichiwa!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter through the logs using the metadata:\n",
        "\n",
        "![](https://github.com/Saif-Shines/pk-cookbook/blob/portkey-on-google-cookbook-repo-klyst-171/integrations/images/resiliency-and-observability-essentials-for-gemini/3-resiliency-and-observability-essentials-for-gemini.png?raw=true)\n",
        "\n",
        "For comprehensive information and more features, see the [Observability docs](https://portkey.ai/docs/product/observability-modern-monitoring-for-llms).\n"
      ],
      "metadata": {
        "id": "hBoCDzCfdzIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Production Reliability\n",
        "\n",
        "Portkey’s multimodal AI gateway lets you tackle failure scenarios and make your app more reliable and robust.\n",
        "\n",
        "\n",
        "\n",
        "1. AI gateway can cache LLM responses and serve them instantly.\n",
        "2. Furthermore, it can enable your app to switch from one LLM to another in case of unexpected failures, ensuring uninterrupted service.\n",
        "3. Additionally, it can distribute incoming traffic evenly among the target LLMs, allowing for efficient load balancing.\n",
        "\n",
        "### Caching, Retries and Request Timeouts\n",
        "\n",
        "It’s easy to enable these features on your requests using [Gateway Configs](https://github.com/Portkey-AI/portkey-cookbook/blob/c0ee5af750b4edc8964339c3bd86adcd83721178/examples).\n",
        "\n",
        "From your **Portkey** app, on **Configs** and write the following JSON:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"retry\": {\n",
        "    \"attempts\": 3\n",
        "  },\n",
        "  \"cache\": {\n",
        "    \"mode\": \"simple\"\n",
        "  },\n",
        "  \"request_timeout\": 10000\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Hit **Save Config** and get a Config ID.\n",
        "\n",
        "![](https://github.com/Saif-Shines/pk-cookbook/blob/portkey-on-google-cookbook-repo-klyst-171/integrations/images/resiliency-and-observability-essentials-for-gemini/4-resiliency-and-observability-essentials-for-gemini.png?raw=true)\n",
        "\n",
        "When making requests to the AI gateway, including the Config ID is essential. This will enable caching, timeouts, and automatic retries.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZiuziHZ-eB7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fallbacks and Loadbalancing\n",
        "\n",
        "Similar to how you added features such as caching for your requests, bringing the power of Fallbacks and Load balancing is just as simple. Both are powerful means to build reliability into your system.\n",
        "\n",
        "Fallbacks\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"strategy\": {\n",
        "    \"mode\": \"fallback\"\n",
        "  },\n",
        "  \"targets\": [\n",
        "    {\n",
        "      \"virtual_key\": \"gemini-virtual-key\",\n",
        "      \"override_params\": {\n",
        "        \"model\": \"gemini-1.0-pro\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"virtual_key\": \"gemini-virtual-key\",\n",
        "      \"override_params\": {\n",
        "        \"model\": \"gemini-1.0-pro-001\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Loadbalancing\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"strategy\": {\n",
        "    \"mode\": \"loadbalance\"\n",
        "  },\n",
        "  \"targets\": [\n",
        "    {\n",
        "      \"virtual_key\": \"gemini-virtual-key\",\n",
        "      \"override_params\": {\n",
        "        \"model\": \"gemini-1.0-pro-latest\"\n",
        "      },\n",
        "      \"weight\": 0.7\n",
        "    },\n",
        "    {\n",
        "      \"virtual_key\": \"gemini-virtual-key\",\n",
        "      \"override_params\": {\n",
        "        \"model\": \"gemini-1.0-pro\"\n",
        "      },\n",
        "      \"weight\": 0.3\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "You can reference them in the code as follows:\n"
      ],
      "metadata": {
        "id": "1768YtbQeodi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_ID=\"LOADBALANCE/FALLBACK_CONFIG_ID\"\n",
        "\n",
        "portkey = Portkey(\n",
        "    api_key=PORTKEY_API_KEY,\n",
        "    virtual_key=GOOGLE_VIRTUAL_KEY,\n",
        "    config=CONFIG_ID # passing Config ID\n",
        ")\n",
        "\n",
        "response = portkey.chat.completions.create(\n",
        "    model=\"gemini-1.0-pro-001\",\n",
        "    messages = [{ \"role\": \"user\", \"content\": \"c'est la vie\" }]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS2SJ-jeVszd",
        "outputId": "409bf4e9-f7b0-465f-8146-868ffb6863a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C'est la vie (translated to \"such is life\") is a French phrase that is often used to express the idea that life is full of unexpected events and that we must accept them with equanimity. It is a reminder that life is not always fair or easy, and that we must learn to embrace both the good and the bad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Improvement\n",
        "\n",
        "Assessing the user impact of your prompts can be a difficult task, as certain prompts may have great user delight while others may lack it.\n",
        "\n",
        "But what if your users could rate the quality of the responses provided by your LLMs? With Portkey, this is possible. By collecting user feedback, you can fine-tune your models and continuously improve their accuracy through autonomous fine-tuning. In other words, user feedback becomes the dataset used to train these models.\n",
        "\n",
        "\n",
        "### Collect Feedback\n",
        "\n",
        "We previously passed a trace ID (`gemini_and_portkey`) in our requests. Portkey’s feedback method can attach feedback to as follows:\n",
        "\n",
        "\n",
        "```py\n",
        "feedback = portkey.feedback.create(\n",
        "    trace_id=\"gemini_and_portkey\",\n",
        "    value=5,   # Integer between -10 and 10\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "Connect any user interaction to record feedback on your response quality. You can also view feedback in the dashboard under the Feedback tab.\n",
        "\n",
        "\n",
        "### Autonomous Fine-tuning\n",
        "\n",
        "Our Fine-Tuning feature can automatically fine-tune models based on feedback. It is currently in a private beta phase. If you are interested, please message us on support@portkey.ai or on our [Discord](https://www.portkey.ai/community) channel.\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Portkey is an excellent tool that applies production-grade features on top of Gemini Models. What we have here is just tip of the iceberg. You can read about more exciting ways to use in[ Portkey Documentation](https://portkey.ai/docs).\n"
      ],
      "metadata": {
        "id": "21GlfFL2e621"
      }
    }
  ]
}