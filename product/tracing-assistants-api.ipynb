{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQw1S7F+QjXe71bjWq9P4C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saif-Shines/pk-cookbook/blob/tracing-assistants-api-klyst-176/product/tracing-assistants-api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tracing Assistants API\n",
        "\n",
        "OpenAI’s Assistants API lets you access tools and persist threads to perform various tasks.\n",
        "\n",
        "Accomplishing a task means multiple moving parts:\n",
        "\n",
        "\n",
        "\n",
        "* Create an `Assistant` that encapsulates the base model, instructions, tools, hyperparameters, and necessary (context) documents.\n",
        "* Create a `Thread` that represents an stateful conversation.\n",
        "* Execute `Runs` on top of `Assistant`s and `Thread`s.\n",
        "\n",
        "This notebook provides a step by step guide to trace all the requests (using [Portkey](https://www.portkey.ai/)) to accomplish a task—making it easy for troubleshooting and debugging.  \n"
      ],
      "metadata": {
        "id": "puR5EB8wnpsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Install OpenAI and Portkey SDKs"
      ],
      "metadata": {
        "id": "zgkatweqntNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmSKeeOpPOBF"
      },
      "outputs": [],
      "source": [
        "!pip install openai portkey-ai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Start by importing them"
      ],
      "metadata": {
        "id": "wIpEPVHjn1pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders\n",
        "from google.colab import userdata\n",
        "import json, time"
      ],
      "metadata": {
        "id": "pzCx58hlP_jl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`PORTKEY_GATEWAY_URL` and `createHeaders` are necessary imports that lets you instantiate `OpenAI` and enable request tracing. Whereas, `json`, `time`, `userdata` are utilities for better logging and handling environment variables (in google colab)."
      ],
      "metadata": {
        "id": "EF5wBY5hoCRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def show_json(obj):\n",
        "    display(json.loads(obj.model_dump_json()))\n",
        "\n",
        "def wait_on_run(run, thread):\n",
        "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "        )\n",
        "        time.sleep(0.5)\n",
        "    return run"
      ],
      "metadata": {
        "id": "MWLbnK8b6onB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Enable tracing at instantiation\n",
        "\n",
        "Tracing is made possible by enabling OpenAI SDK send requests through Portkey.\n",
        "\n"
      ],
      "metadata": {
        "id": "dyQSCRFUoGXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=userdata.get('OPENAI_API_KEY'), # instead virtual key is respected\n",
        "    base_url=PORTKEY_GATEWAY_URL,\n",
        "    default_headers=createHeaders(\n",
        "        provider=\"openai\",\n",
        "        api_key=userdata.get('PORTKEY_API_KEY'),\n",
        "        virtual_key=userdata.get('OPENAI_VIRTUAL_KEY'),\n",
        "        trace_id=\"assistants\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "_cBeEU5-RK_h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Portkey’s gateway URL is set as `base_url` where requests are sent through.\n",
        "\n",
        "The `default_headers` contains the headers that directs requests to the target Large Langauage Models (LLMs).\n",
        "\n",
        "* **Trace ID:** Specify any desirable string to trace through the logs.\n",
        "* **Virtual Keys:** Alternative to using original API keys once secured in the Portkey app. See [docs](https://portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/virtual-keys).\n",
        "* **API Key:** Specify the [Portkey API key](https://portkey.ai/docs/welcome/make-your-first-request#id-1.-get-your-portkey-api-key).\n",
        "\n",
        "Among the top level `api_key` and `virtual_key` (in default headers), the `virtual_key` (when present) is respected.\n"
      ],
      "metadata": {
        "id": "apnISkxsoQXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  2. Tools: Prepare for File Search\n",
        "\n",
        "This notebook demonstrates how to trace as you use Assistants API to retrieve an PDF document and generate a tweet thread based on it.\n",
        "\n",
        "Upload the PDF document and get an File ID to reference it in the Vector Store."
      ],
      "metadata": {
        "id": "2y1fx1KOonoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = client.files.create(\n",
        "    file=open(\"prompt-partials.pdf\", \"rb\"), # upload in local any PDF\n",
        "    purpose=\"assistants\"\n",
        "  )\n",
        "\n",
        "file_id = file.id\n",
        "\n",
        "print(file_id)"
      ],
      "metadata": {
        "id": "l0FY643lRWxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace66c1c-46db-4d45-e2d2-39bb811b8b6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file-cdoQlBYIVayRBJoqqLUAk6qA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = client.beta.vector_stores.create(\n",
        "  name=\"Documents\",\n",
        "  file_ids=[file_id]\n",
        ")\n",
        "\n",
        "print(vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6Ep09hmHDCv",
        "outputId": "9d5d8ad6-ae1f-4fbf-e106-9f1b708ad403"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VectorStore(id='vs_El8BQ75OkBpcbdYSfCpgFEJB', created_at=1714407352, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=1, total=1), last_active_at=1714407352, metadata={}, name='Documents', object='vector_store', status='in_progress', usage_bytes=0, expires_after=None, expires_at=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create an `Assistant` and a `Thread`\n",
        "\n",
        "Instruct the assistant to convert the document into twitter threads.\n"
      ],
      "metadata": {
        "id": "XKGxwKcjpHrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Y Threads Converter\",\n",
        "    instructions=\"You are a helpful assistant. Given a set of files, you extract the most interesting information and restructure it into Threads format for Twitter without asking any further questions.\",\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    tools=[{\"type\": \"file_search\"}], # See supported tools on OpenAI docs\n",
        "    tool_resources={\n",
        "        \"file_search\": {\n",
        "            \"vector_store_ids\": [vector_store.id]\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "assistant_id = assistant.id\n",
        "print(assistant_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42i83u8fST3L",
        "outputId": "f50cdd45-a7da-4f2b-b69a-008129e0cd59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asst_NXAwYqPblhNMWee7D1yLler5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `show_json(..)` utility helps to log response in JSON for better readability."
      ],
      "metadata": {
        "id": "7H5BGbOypaWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_json(obj):\n",
        "    display(json.loads(obj.model_dump_json()))"
      ],
      "metadata": {
        "id": "kgka2D2HpbFA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next up create a thread and add a user message."
      ],
      "metadata": {
        "id": "W6V-neQTph2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()\n",
        "\n",
        "thread_id = thread.id"
      ],
      "metadata": {
        "id": "bswjHmVZVioi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Create a X Thread\",\n",
        ")\n",
        "show_json(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "HUy_RIoYsSxl",
        "outputId": "4c12d8c2-0b94-4b84-e625-fcb839fa1bcc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'msg_qYldKBB2WYUncBAxH3gf9QU5',\n",
              " 'assistant_id': None,\n",
              " 'attachments': [],\n",
              " 'completed_at': None,\n",
              " 'content': [{'text': {'annotations': [], 'value': 'Create a X Thread'},\n",
              "   'type': 'text'}],\n",
              " 'created_at': 1714407520,\n",
              " 'incomplete_at': None,\n",
              " 'incomplete_details': None,\n",
              " 'metadata': {},\n",
              " 'object': 'thread.message',\n",
              " 'role': 'user',\n",
              " 'run_id': None,\n",
              " 'status': None,\n",
              " 'thread_id': 'thread_vhCz2rOYT8qWwws1H4XaczfP'}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Perform the Task with a `Run`\n",
        "\n",
        "Creating a `run` means to allow an `assistant` to execute on a `thread`. This asynchronous operation returns `run`’s metadata immediately where `status` is set to `queued`.\n",
        "\n",
        "The `wait_on_run(..)` utility ensures the `run` is `completed` by polling the `run` in a loop."
      ],
      "metadata": {
        "id": "lLwH96b3pwsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_on_run(run, thread):\n",
        "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "        )\n",
        "        time.sleep(0.5)\n",
        "    return run"
      ],
      "metadata": {
        "id": "XnhhcwT-p4AO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute a `run`"
      ],
      "metadata": {
        "id": "_yiQS6wMp7St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread_id,\n",
        "  assistant_id=assistant_id\n",
        ")\n",
        "\n",
        "run_id = run.id\n",
        "\n",
        "run = wait_on_run(run, thread)\n",
        "show_json(run)"
      ],
      "metadata": {
        "id": "pmJZb2YMWUlZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "a0b469f2-0f84-4195-d285-99e9293a701b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'run_bWUOwm5oD3dkHIb3KLMa538b',\n",
              " 'assistant_id': 'asst_NXAwYqPblhNMWee7D1yLler5',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': 1714407639,\n",
              " 'created_at': 1714407603,\n",
              " 'expires_at': None,\n",
              " 'failed_at': None,\n",
              " 'incomplete_details': None,\n",
              " 'instructions': 'You are a helpful assistant. Given a set of files, you extract the most interesting information and restructure it into Threads format for Twitter without asking any further questions.',\n",
              " 'last_error': None,\n",
              " 'max_completion_tokens': None,\n",
              " 'max_prompt_tokens': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-4-1106-preview',\n",
              " 'object': 'thread.run',\n",
              " 'required_action': None,\n",
              " 'response_format': 'auto',\n",
              " 'started_at': 1714407603,\n",
              " 'status': 'completed',\n",
              " 'thread_id': 'thread_vhCz2rOYT8qWwws1H4XaczfP',\n",
              " 'tool_choice': 'auto',\n",
              " 'tools': [{'type': 'file_search'}],\n",
              " 'truncation_strategy': {'type': 'auto', 'last_messages': None},\n",
              " 'usage': {'completion_tokens': 529,\n",
              "  'prompt_tokens': 2272,\n",
              "  'total_tokens': 2801},\n",
              " 'temperature': 1.0,\n",
              " 'top_p': 1.0,\n",
              " 'tool_resources': {}}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. List the messages in the thread\n",
        "\n",
        "Depending on the instructions and user messages, the assistant may perform the task or ask answers for more questions. Prompt the user for additional inputs to give assistant the sufficient context.\n",
        "\n",
        "In this example, the instructions include not to ask further questions. List the messages on the thread that includes the tweets.\n"
      ],
      "metadata": {
        "id": "zuQ1NlDrqW9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread_messages = client.beta.threads.messages.list(thread_id)\n",
        "print(thread_messages.data[0].content[0].text.value)\n",
        "# show_json(thread_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DadRwJ6Wn6a",
        "outputId": "414681d5-1da6-48b2-82a6-e0bb6a7ebdd6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a Twitter Thread using Portkey Prompt Partials:\n",
            "\n",
            "🧵1/ X Thread:\n",
            "Let's dive into how prompt partials revolutionize prompt creation on Portkey app. Imagine crafting diverse prompts with a common core—simple, efficient, and effective!\n",
            "\n",
            "2/ Prerequisites:\n",
            "Before harnessing the power of prompt partials, ensure you've integrated OpenAI with your Portkey account. Secure your API keys and prep for a smooth workflow: https://www.portkey.ai.\n",
            "\n",
            "3/ Installing Portkey SDK:\n",
            "Jumpstart your journey with the following command:\n",
            "```\n",
            "npm install portkey-ai\n",
            "```\n",
            "\n",
            "4/ Crafting Partial Prompts:\n",
            "A building block for multipurpose prompts—define once, use anywhere!\n",
            "\n",
            "Here’s a guide to creating a partial prompt:\n",
            "Navigate to Prompts > Prompt Partials (Tab) > Create.\n",
            "\n",
            "5/ Mustache Syntax for Variables:\n",
            "Employ mustache syntax to dynamically insert values into your prompts. A unique partial ID locks the blueprint for reuse.\n",
            "\n",
            "6/ Designing Prompt Templates:\n",
            "The Prompt Playground on Portkey lets you experiment and finalize the perfect model interaction template for your needs.\n",
            "\n",
            "7/ Example - Address Inquiry:\n",
            "Reference partials in templates to personalize user experiences with ease. Variables like branch_id and street_address transform static prompts into dynamic conversations.\n",
            "\n",
            "8/ Triggering the Prompt:\n",
            "Unleash the power of your prompt template with Portkey's SDK. Witness how a facilities-related query comes to life with just a few lines of code.\n",
            "\n",
            "```js\n",
            "// Sample code snippet\n",
            "import Portkey from 'portkey-ai';\n",
            "// ... Variable declarations\n",
            "const promptCompletion = await portkey.prompts.completions.create({/*...*/});\n",
            "console.log(promptCompletion.choices[0].message.content);\n",
            "```\n",
            "\n",
            "9/ Real-time Solutioning:\n",
            "Get instant, contextual responses like cafeteria directions, tinged with personal touches—a classic showcase of the Prompt Partial prowess.\n",
            "\n",
            "Enjoy the seamless integration and dynamic response generation that Portkey offers. Revolutionize your prompt creation process with Portkey's Prompt Partials!\n",
            "\n",
            "Learn more here: https://docs.portkey.ai/docs/product/ai-gateway-streamline-llm-integrations/virtual-keys and https://www.tsmean.com/articles/mustache/the-ultimate-mustache-tutorial/.\n",
            "\n",
            "That's a wrap on the wonders of Portkey Prompt Partials! \n",
            "\n",
            "✨ Stay tuned for more innovative prompt engineering insights! ✨\n",
            "\n",
            "#Portkey #PromptPartials #AIIntegration #DynamicPrompts\n",
            "\n",
            "End of 🧵\n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Tracing and Logging\n",
        "\n",
        "Input `assistants` as **Trace Id **in the search bar to list all the logs specific to the current Assistant API. The `PATH` column indicates the nature of the operations - creating files, runs, messages, retrieving runs, and listing messages.\n",
        "\n",
        "![](https://raw.githubusercontent.com/Portkey-AI/portkey-cookbook/be59727ffedf76be581c24689cd27cc596f5b521/product/images/tracing-assistants-api/1-tracing-assistants-api.gif)\n",
        "\n",
        "All the request and response logs are available for you to quickly identify and investigate any issues when you encounter them.\n"
      ],
      "metadata": {
        "id": "KByrAXp4qflw"
      }
    }
  ]
}